{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_info():\n",
    "    # Set up Splinter\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    # Path to Mars website.\n",
    "    url = \"https://redplanetscience.com/\"\n",
    "    browser.visit(url)\n",
    "    \n",
    "    # Pause to let the page load.\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Get the latest News Title\n",
    "    # From inspect I found: <div class = \"content_title\">\n",
    "    news_title = soup.find('div', class_= \"content_title\").text\n",
    "\n",
    "    # Get the latest News Paragraph text\n",
    "    # From inspect I found: <div class=\"article_teaser_body\">\n",
    "    news_p = soup.find('div', class_= \"article_teaser_body\").text\n",
    "\n",
    "    # Path to Mars image page.\n",
    "    url_2 = \"https://spaceimages-mars.com/\"\n",
    "    browser.visit(url_2)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Slooooowwwwww down. What's the rush?\n",
    "    time.sleep(1)  \n",
    "\n",
    "    # Find the src for the featured Mars image.\n",
    "    # The main image is the second one featured.\n",
    "    img_source = soup.find('img', {\"class\" : \"headerimage fade-in\"})[\"src\"]\n",
    "\n",
    "    mars_img = url_2 + img_source\n",
    "\n",
    "    # Pathing to Mars facts page.\n",
    "    url_3 = \"https://galaxyfacts-mars.com/\"\n",
    "\n",
    "    # Using Pandas to scrape the tables.\n",
    "    tables = pd.read_html(url_3)\n",
    "\n",
    "    # Grabbing the first table.\n",
    "    mars_df = tables[0] \n",
    "\n",
    "    # Table has a formatted header. Removing it to make the first row the header instead.\n",
    "    header = mars_df.iloc[0]\n",
    "    mars_df = mars_df[1:]\n",
    "    mars_df.columns = header\n",
    "    mars_df.set_index('Mars - Earth Comparison',inplace=True)\n",
    "\n",
    "    # Saving the dataframe to html.\n",
    "    mars_facts = mars_df.to_html()\n",
    "\n",
    "    # Path to Mars website.\n",
    "    url_4 = \"https://marshemispheres.com/\"\n",
    "    browser.visit(url_4)\n",
    "    \n",
    "    # Pause to let the page load.\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # Creating empty lists to help store the data for the 4 hemispheres.\n",
    "    title = []\n",
    "    links = []\n",
    "    img_url = []\n",
    "\n",
    "    # Finding all the separate listings for each hemisphere.\n",
    "    for div in soup.find_all('div', {\"class\" : \"description\"}):\n",
    "        # Grabbing the title of each hemisphere.\n",
    "        img_title = div.find('h3').get_text()\n",
    "        title.append(img_title)\n",
    "    \n",
    "        # This is where things get tricky. The full-sized high, quality image was at another linked website.\n",
    "        # This grabs those links.\n",
    "        img_link = div.find('a')['href']\n",
    "        links.append(img_link)\n",
    "\n",
    "    # Unless I completely overlooked something I have to path to each site to grab the full-sized images.\n",
    "    # This loops through the links to do just that.\n",
    "    for link in links:\n",
    "        # Path to each hemisphere's full sized-image, they are modifications of the original site.\n",
    "        hemisphere_url = url_4 + link\n",
    "        browser.visit(hemisphere_url)\n",
    "        \n",
    "        # Pause to let the page load\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Scrape each page into Soup\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        full_img_path = soup.find('img', {\"class\" : \"wide-image\"})[\"src\"]\n",
    "\n",
    "        # Wouldn't you believe it? Another link.\n",
    "        # Pathing to each hemisphere's full sized-image. \n",
    "        hemisphere_img_url = url_4 + full_img_path\n",
    "        browser.visit(hemisphere_img_url)\n",
    "                \n",
    "        # Pause to let the page load\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Scrape this final page into Soup\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # The image is the only one featured here so this is easier.\n",
    "        # Storing the picture into the list.\n",
    "        current_img = soup.find('img')[\"src\"]\n",
    "        img_url.append(current_img)\n",
    "\n",
    "    # Store data in a dictionary\n",
    "    mars_info = {\n",
    "        \"mars_img\": mars_img,\n",
    "        \"news_title\": news_title,\n",
    "        \"news_p\": news_p,\n",
    "        \"mars_facts\" : mars_facts,\n",
    "        \"hemi_title1\" : title[0],\n",
    "        \"hemi_image1\" : img_url[0],\n",
    "        \"hemi_title2\" : title[1],\n",
    "        \"hemi_image2\" : img_url[1],\n",
    "        \"hemi_title3\" : title[2],\n",
    "        \"hemi_image3\" : img_url[2],\n",
    "        \"hemi_title4\" : title[3],\n",
    "        \"hemi_image4\" : img_url[3]\n",
    "    }\n",
    "\n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "\n",
    "    # Return results\n",
    "    return mars_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\marcu\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mars_img': 'https://spaceimages-mars.com/image/featured/mars3.jpg',\n",
       " 'news_title': \"Three New Views of Mars' Moon Phobos\",\n",
       " 'news_p': \"Taken with the infrared camera aboard NASA's Odyssey orbiter, they reveal temperature variations on the small moon as it drifts into and out of Mars’ shadow.\",\n",
       " 'mars_facts': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Mars</th>\\n      <th>Earth</th>\\n    </tr>\\n    <tr>\\n      <th>Mars - Earth Comparison</th>\\n      <th></th>\\n      <th></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>Diameter:</th>\\n      <td>6,779 km</td>\\n      <td>12,742 km</td>\\n    </tr>\\n    <tr>\\n      <th>Mass:</th>\\n      <td>6.39 × 10^23 kg</td>\\n      <td>5.97 × 10^24 kg</td>\\n    </tr>\\n    <tr>\\n      <th>Moons:</th>\\n      <td>2</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>Distance from Sun:</th>\\n      <td>227,943,824 km</td>\\n      <td>149,598,262 km</td>\\n    </tr>\\n    <tr>\\n      <th>Length of Year:</th>\\n      <td>687 Earth days</td>\\n      <td>365.24 days</td>\\n    </tr>\\n    <tr>\\n      <th>Temperature:</th>\\n      <td>-87 to -5 °C</td>\\n      <td>-88 to 58°C</td>\\n    </tr>\\n  </tbody>\\n</table>',\n",
       " 'hemi_title1': 'Cerberus Hemisphere Enhanced',\n",
       " 'hemi_image1': 'https://marshemispheres.com/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg',\n",
       " 'hemi_title2': 'Schiaparelli Hemisphere Enhanced',\n",
       " 'hemi_image2': 'https://marshemispheres.com/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg',\n",
       " 'hemi_title3': 'Syrtis Major Hemisphere Enhanced',\n",
       " 'hemi_image3': 'https://marshemispheres.com/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg',\n",
       " 'hemi_title4': 'Valles Marineris Hemisphere Enhanced',\n",
       " 'hemi_image4': 'https://marshemispheres.com/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_info()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
